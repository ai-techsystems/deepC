// Copyright 2018 The AITS DNNC Authors.All Rights Reserved.
//
// Licensed to the Apache Software Foundation(ASF) under one
// or more contributor license agreements.See the NOTICE file
// distributed with this work for additional information
// regarding copyright ownership.The ASF licenses this file
// to you under the Apache License, Version 2.0 (the
// "License"); you may not use this file except in compliance
// with the License.You may obtain a copy of the License at
//
//	 http://www.apache.org/licenses/LICENSE-2.0
//
// Unless required by applicable law or agreed to in writing,
// software distributed under the License is distributed on an
// "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
// KIND, either express or implied.See the License for the
// specific language governing permissionsand limitations
// under the License.
//
// This file is part of AITS DNN compiler maintained at
// https://github.com/ai-techsystems/dnnCompiler

#include <Python.h>

#include "core/tensor.h"
#include "operators/Add.h"
#include "operators/DequantizeLinear.h"
#include "operators/Div.h"
#include "operators/Elu.h"
#include "operators/Equal.h"
#include "operators/Erf.h"
#include "operators/Exp.h"
#include "operators/EyeLike.h"
#include "operators/Flatten.h"
#include "operators/Floor.h"
#include "operators/Gemm.h"
#include "operators/GlobalAveragePool.h"
#include "operators/GlobalLpPool.h"
#include "operators/GlobalMaxPool.h"
#include "operators/Greater.h"
#include "operators/HardSigmoid.h"
#include "operators/Hardmax.h"
#include "operators/Identity.h"
#include "operators/InstanceNormalization.h"
#include "operators/IsInf.h"
#include "operators/IsNaN.h"
#include "operators/LRN.h"
#include "operators/LeakyRelu.h"
#include "operators/Less.h"
#include "operators/Log.h"
#include "operators/LogSoftmax.h"
#include "operators/LpNormalization.h"
#include "operators/MatMul.h"
#include "operators/MatMulInteger.h"
#include "operators/Max.h"
#include "operators/Mean.h"
#include "operators/Min.h"
#include "operators/ThresholdedRelu.h"
#include "operators/Transpose.h"

extern std::vector<float> listTupleToVector_Float(PyObject *);
extern std::vector<size_t> listTupleToVector_SizeT(PyObject *);

using namespace dnnc;

/*## (Don't remove the '##', it indicates the split line)
The code above this commented block will be copied as it is. 
Keep in mind, this file is "\n" and "whitespace" sensitive,
so try to keep the structure similar. 
Do not comment out any line in between 2 operator functions.
Every operator function is seperated by a blank line.
So don't leave empty line anywhere else other than in between 2 operators.
The code below will be modified according to the dtype.
DO NOT NAME TENSOR VARIABLES AS 'INPUT' go for 'a', 'b' etc
Note: Not all of them are modified to input output format
I have updated my part, you should update yours too. */

tensor<float> array(size_t x, size_t y = 0, size_t z = 0, size_t w = 0) {
	return tensor<float>(x, y, z, w, "", dnnc::INIT_RANDOM);
}

tensor<float> empty(size_t x, size_t y = 0, size_t z = 0, size_t w = 0) {
	return tensor<float>(x, y, z, w, "", dnnc::INIT_NONE);
}

tensor<float> zeros(size_t x, size_t y = 0, size_t z = 0, size_t w = 0) {
	return tensor<float>(x, y, z, w, "", dnnc::INIT_ZERO);
}

tensor<float> ones(size_t x, size_t y = 0, size_t z = 0, size_t w = 0) {
	return tensor<float>(x, y, z, w, "", dnnc::INIT_ONE);
}

tensor<float> random(size_t x, size_t y = 0, size_t z = 0, size_t w = 0) {
	return tensor<float>(x, y, z, w, "", dnnc::INIT_RANDOM);
}

tensor<float> array(PyObject *objects) {
	if (!PyList_Check(objects))
		throw std::logic_error("array expects list to create array.\n");
	Py_ssize_t sz = PyList_Size(objects);
	if (!sz)
		throw std::logic_error("array expects list of non-zero size.\n");
	PyObject *a_list = PyList_GetItem(objects, 0);
	if (PyList_Check(a_list)) {
		// 2D: dc.array(([1,2],[10,20]]);
		// Py_ssize_t sz = PyList_Size(a_list);
		std::vector<size_t> vDims;
		std::vector<float> vContents;
		for (Py_ssize_t i = 0; i < sz; i++) {
			std::vector<float> aVec =
					listTupleToVector_Float(PyList_GetItem(objects, i));
			vDims.push_back(aVec.size());
			vContents.insert(vContents.end(), aVec.begin(), aVec.end());
		}
		// make sure VDims are consistent.
		for (size_t i = 1; i < vDims.size(); i++) {
			if (vDims[0] != vDims[i])
				throw std::logic_error(
						"2D array expects list of lists with same size.");
		}
		// fill it as row major data.
		tensor<float> result(vDims.size(), vDims[0]);
		result.load(vContents);
		return result;
	} else {
		// 1D: dc.array([2,3,4])
		std::vector<float> aVec = listTupleToVector_Float(objects);
		tensor<float> result(aVec.size());
		result.load(aVec);
		return result;
	}
	return tensor<float>();
}

tensor<float> arange(size_t stop, size_t start = 0, size_t step = 1) {
	// swap if the range is invalid.
	if (stop < start) {
		size_t tmp = start;
		start = stop;
		stop = tmp;
	}
	if (stop == start)
		throw std::logic_error("arange expects stop arg more than start.\n");
	std::vector<float> vContents;
	for (size_t i = start; i < stop; i = i + step)
		vContents.push_back(static_cast<float>(i));
	tensor<float> result(vContents.size());
	result.load(vContents);
	return result;
}

tensor<output> reshape(tensor<input> &tens_obj, PyObject *newShape) {
	tensor<output> result ;
	if (PyLong_Check(newShape)) {
		std::vector<size_t> nShape;
		auto newShapeMember = PyLong_AsLong(newShape);
		nShape.push_back(static_cast<size_t>(newShapeMember));
		if (nShape[0] <= 0)
			throw std::logic_error("reshape integer must have positive value.\n");
		result = tens_obj.reshape(nShape);
	    return result;
	} else if (PyTuple_Check(newShape)) {
		auto vShape = listTupleToVector_SizeT(newShape);
		for (size_t i = 0; i < vShape.size(); i++)
			if (vShape[i] <= 0)
				throw std::logic_error("reshape tupel must have positive elements.\n");
		result = tens_obj.reshape(vShape);
	    return result;
	} else {
		throw std::logic_error("reshape type must be int or tuple.\n");
    }
	return tens_obj;
	dtype = {
		"double" : "double",
		"float" : "float",
		"int" : "int",
		"bool" : "bool"
	}
}

tensor<output> matmul(tensor<input> &a, tensor<input> &b) {
	MatMul<input> op;
	return op.compute(a, b);
	dtype = {
		"double" : "double",
		"float" : "float",
		"int" : "int"
	}
}

tensor<output> add(tensor<input> &a, tensor<input> &b) {
	Add<input> op;
	return op.compute(a, b);
	dtype = {
		"double" : "double",
		"float" : "float",
		"int" : "int"
	}
}

tensor<output> dequantize_linear(tensor<input> &a, tensor<float> &b, tensor<input> &c) {
	DequantizeLinear<input> op;
	return op.compute(a, b, c);
	dtype = {
		"float" : "float"
	}
}

tensor<output> div(tensor<input> &a, tensor<input> &b) {
	Div<input> op;
	return op.compute(a, b);
	dtype = {
		"double" : "double",
		"float" : "float",
		"int" : "int"
	}
}

tensor<output> elu(tensor<input> &a, float alpha = 1.0) {
	Elu<input> op("localOpName", alpha);
	return op.compute(a);
	dtype = {
		"double" : "double",
		"float" : "float"
	}
}

tensor<output> equal(tensor<input> &a, tensor<input> &b) {
	Equal<input> op;
	return op.compute(a, b);
	dtype = {
		"double" : "bool",
		"float" : "bool",
		"bool" : "bool",
		"int" : "bool"
	}
}

tensor<output> erf(tensor<input> &a) {
	Erf<input> op;
	return op.compute(a);
	dtype = {
		"double" : "double",
		"float" : "float"
	}
}

tensor<output> exp(tensor<input> &a) {
	Exp<input> op;
	return op.compute(a);
	dtype = {
		"double" : "double",
		"float" : "float"
	}
}

tensor<output> eye_like(tensor<input> &a, int k = 0) {
	EyeLike<input> op("localOpName", k);
	return op.compute(a);
	dtype = {
		"double" : "double",
		"float" : "float",
		"int" : "int",
		"bool" : "bool",
	}
}

tensor<output> flatten(tensor<input> &a, int axis = 1) {
	Flatten<input> op("localOpName", axis);
	return op.compute(a);
	dtype = {
		"double" : "double",
		"float" : "float",
		"int" : "int",
		"bool" : "bool"
	}
}

tensor<output> floor(tensor<input> &a) {
	Floor<input> op;
	return op.compute(a);
	dtype = {
		"double" : "double",
		"float" : "float"
	}
}

tensor<output> gemm(tensor<input> &a, tensor<input> &b, tensor<input> &c, float alpha = 1.0, float beta = 1.0, int transA = 0, int transB = 0) {
	Gemm<input> op("localOpName", alpha, beta, transA, transB);
	return op.compute(a, b, c);
	dtype = {
		"double" : "double",
		"float" : "float"
	}
}

tensor<input> global_average_pool(tensor<input> &a) {
  GlobalAveragePool<input> op;
  return op.compute(a);
  dtype = {
    "float" : "float",
    "double" : "double"
  }
}

tensor<output> global_lp_pool(tensor<input> &a, int p = 2) {
  GlobalLpPool<input> op("localOpName", p);
  return op.compute(a);
  dtype = {
    "float" : "float",
    "double" : "double"
  }
}

tensor<output> global_max_pool(tensor<input> &a) {
  GlobalMaxPool<input> op;
  return op.compute(a);
  dtype = {
    "float" : "float",
    "double" : "double"
  }
}

tensor<output> greater(tensor<input> &a, tensor<input> &b) {
  Greater<input> op;
  return op.compute(a, b);
  dtype = {
    "int" : "bool",
    "float" : "bool",
    "double" : "bool"
  }
}

tensor<output> hardmax(tensor<input> &a, int axis = 0) {
  Hardmax<input> op("localOpName", axis);
  return op.compute(a);
  dtype = {
    "float" : "float",
    "double" : "double"
  }
}

tensor<output> hardsigmoid(tensor<input> &a, float alpha = 0.2,float beta = 0.5) {
  HardSigmoid<input> op("localOpName", alpha, beta);
  return op.compute(a);
  dtype = {
    "float" : "float",
    "double" : "double"
  }
}

tensor<output> identity(tensor<input> &a) {
  Identity<input> op;
  return op.compute(a);
  dtype = {
    "bool" : "bool",
    "int" : "int",
    "float" : "float",
    "double" : "double"
  }
}

tensor<output> instancenormalization(tensor<input> &a, tensor<input> &scale,tensor<input> &B, float epsilon = 1e-5) {
  InstanceNormalization<input> op("localOpName", epsilon);
  return op.compute(a, scale, B);
  dtype = {
    "float" : "float",
    "double" : "double"
  }
}

tensor<output> isinf(tensor<input> &a, int detect_positive = 1,int detect_negative = 1) {
  IsInf<input> op("localOpName", detect_positive, detect_negative);
  return op.compute(a);
  dtype = {
    "float" : "bool",
    "double" : "bool"
  }
}

tensor<output> isnan(tensor<input> &a) {
  IsNaN<input> op;
  return op.compute(a);
  dtype = {
    "float" : "bool",
    "double" : "bool"
  }
}

tensor<output> lrn(tensor<input> &a, int size, float alpha = 0.0001,float beta = 0.75, float bias = 1.0) {
  LRN<input> op(size, "localOpName", alpha, beta, bias);
  return op.compute(a);
  dtype = {
    "float" : "float",
    "double" : "double"
  }
}

tensor<output> leakyrelu(tensor<input> &a, float alpha = 0.01) {
  LeakyRelu<input> op("localOpName", alpha);
  return op.compute(a);
  dtype = {
    "float" : "float",
    "double" : "double"
  }
}

tensor<output> less(tensor<input> &a, tensor<input> &b) {
  Less<input> op;
  return op.compute(a, b);
  dtype = {
    "int" : "bool",
    "float" : "bool",
    "double" : "bool"
  }
}

tensor<output> log(tensor<input> &a) {
  Log<input> op;
  return op.compute(a);
  dtype = {
    "float" : "float",
    "double" : "double"
  }
}

tensor<output> lpnormalization(tensor<input> &a, int p = 2, int axis = -1) {
  LpNormalization<input> op("localOpName", p, axis);
  return op.compute(a);
  dtype = {
    "float" : "float",
    "double" : "double"
  }
}

tensor<output> matmulinteger(tensor<input> &a, tensor<input> &b) {
	MatMulInteger<input> op;
	return op.compute(a, b);
  dtype = {
	"int" : "int"
  }
}

tensor<output> min(std::vector<tensor<input>> inputs) {
  Min<input> op;
  return op.compute(inputs);
  dtype = {
    "float" : "float",
    "double" : "double"
  }
}

tensor<output> mean(std::vector<tensor<input>> inputs) {
  Mean<input> op;
  return op.compute(inputs);
  dtype = {
    "float" : "float",
    "double" : "double"
  }
}

tensor<output> max(std::vector<tensor<input>> inputs) {
  Max<input> op;
  return op.compute(inputs);
  dtype = {
    "float" : "float",
    "double" : "double"
  }
}

/* The below operators need to change accroding to above operators */

tensor<float> thresholded_relu(tensor<float> &a) {
	ThresholdedRelu<float> op;
	return op.compute(a);
}

tensor<float> transpose(tensor<float> &a) {
	dnnc::Transpose<float> op;
	return op.compute(a);
}
